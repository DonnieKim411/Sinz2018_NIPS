{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nips2018.utils import set_seed\n",
    "from nips2018.utils.measures import corr\n",
    "from collections import namedtuple\n",
    "from nips2018.movie.parameters import DataConfig, Seed\n",
    "from nips2018.architectures.readouts import SpatialTransformerPooled3dReadout, ST3dSharedGridStopGradientReadout\n",
    "from nips2018.architectures.cores import StackedFeatureGRUCore, Stacked3dCore\n",
    "from nips2018.architectures.shifters import StaticAffineShifter\n",
    "from nips2018.architectures.modulators import GateGRUModulator \n",
    "from nips2018.movie import data\n",
    "from nips2018.movie.models import Encoder\n",
    "from nips2018.architectures.base import CorePlusReadout3d\n",
    "import torch\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "from attorch.layers import elu1, Elu1\n",
    "from attorch.train import early_stopping, cycle_datasets\n",
    "\n",
    "from attorch.dataset import to_variable\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "from itertools import chain, repeat\n",
    "from attorch.losses import PoissonLoss3d\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(models, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    \n",
    "def load_checkpoint(model, filename):\n",
    "    statedict = torch.load(filename)\n",
    "    model.load_state_dict(statedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MovieMultiDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataConfig.AreaLayerClipRawInputResponse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = dict(data_hash='5253599d3dceed531841271d6eeba9c5',\n",
    "           group_id=22,\n",
    "           seed=2606\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPU = torch.cuda.device_count()\n",
    "N_GPU = 1\n",
    "batch_size= 5\n",
    "val_subsample = None #1000\n",
    "n_subsample=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(key['seed'])\n",
    "\n",
    "trainsets, trainloaders = DataConfig().load_data(key, tier='train', batch_size=batch_size)\n",
    "n_neurons = OrderedDict([(k, v.n_neurons) for k, v in trainsets.items()])\n",
    "valsets, valloaders = DataConfig().load_data(key, tier='validation', batch_size=1, key_order=trainsets)\n",
    "\n",
    "testsets, testloaders = DataConfig().load_data(key, tier='test', batch_size=2, key_order=trainsets)\n",
    "img_shape = list(trainloaders.values())[0].dataset.img_shape\n",
    "trainsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nips2018.movie import parameters\n",
    "best = Encoder * (dj.U('group_id').aggr(Encoder, best = 'max(val_corr)')) & 'val_corr >= best and group_id=22'\n",
    "best * parameters.CoreConfig.StackedFeatureGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = StackedFeatureGRUCore(input_channels=img_shape[1], hidden_channels=12, rec_channels=36,\n",
    "                    input_kern=7, hidden_kern=3, rec_kern=3, layers=3, \n",
    "                    gamma_input=50, gamma_hidden=.1, gamma_rec=.0, momentum=.1,\n",
    "                             skip=2, bias=False, batch_norm=True, pad_input=True\n",
    "                   )\n",
    "ro_in_shape = CorePlusReadout3d.get_readout_in_shape(core, img_shape)\n",
    "\n",
    "readout = ST3dSharedGridStopGradientReadout(ro_in_shape, \n",
    "                                               n_neurons, \n",
    "                                               positive=False,  \n",
    "                                               gamma_features=1., \n",
    "                                               pool_steps=2,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=4,\n",
    "                                            gradient_pass_mod=3\n",
    "                                           )\n",
    "shifter = StaticAffineShifter(n_neurons, input_channels=2, hidden_channels=2, bias=True, gamma_shifter=0.001)\n",
    "modulator = GateGRUModulator(n_neurons, gamma_modulator=0.0, hidden_channels=50, offset=1, bias=True)\n",
    "model = CorePlusReadout3d(core, readout, nonlinearity=Elu1(), \n",
    "                        shifter=shifter, modulator=modulator, burn_in=15)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = list(trainloaders.values())[0].dataset.img_shape\n",
    "\n",
    "\n",
    "\n",
    "criterion = PoissonLoss3d()\n",
    "n_datasets = len(trainloaders)\n",
    "acc = 1 # accumulate gradient over this many steps\n",
    "\n",
    "\n",
    "# --- setup objective\n",
    "grad_passes = 0\n",
    "for ro in model.readout.values():\n",
    "    grad_passes += int(not ro.stop_grad)\n",
    "\n",
    "def full_objective(model, readout_key, inputs, beh, eye_pos, targets):\n",
    "    outputs = model(inputs, readout_key, eye_pos=eye_pos, behavior=beh)\n",
    "    return (criterion(outputs, targets)\n",
    "            + (model.core.regularizer() / grad_passes if not model.readout[readout_key].stop_grad else 0)\n",
    "            + model.readout.regularizer(readout_key).cuda(0)\n",
    "            + (model.shifter.regularizer(readout_key) if model.shift else 0)\n",
    "            + (model.modulator.regularizer(readout_key) if model.modulate else 0)) / acc\n",
    "\n",
    "# --- initialize\n",
    "stop_closure = Encoder().get_stop_closure(valloaders, subsamp_size=val_subsample)\n",
    "\n",
    "mu_dict = OrderedDict([\n",
    "    (k, dl.dataset.mean_trial().responses) for k, dl in trainloaders.items()\n",
    "])\n",
    "model.readout.initialize(mu_dict)\n",
    "model.core.initialize()\n",
    "\n",
    "\n",
    "if model.shifter is not None:\n",
    "    biases = OrderedDict([\n",
    "        (k, -dl.dataset.mean_trial().eye_position) for k, dl in trainloaders.items()\n",
    "    ])\n",
    "    model.shifter.initialize(bias=biases)\n",
    "if model.modulator is not None:\n",
    "    model.modulator.initialize()\n",
    "\n",
    "model = model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, objective, optimizer, stop_closure, trainloaders, epoch=0, post_epoch_hook=None,\n",
    "          interval=1, patience=10, max_iter=10, maximize=True, tolerance=1e-6, cuda=True,\n",
    "          restore_best=True, accumulate_gradient=1):\n",
    "    assert not isinstance(optimizer, torch.optim.LBFGS), \"We don't BFGS at the moment. \"\n",
    "    optimizer.zero_grad()\n",
    "    iteration = 0\n",
    "    assert accumulate_gradient > 0, 'accumulate_gradient needs to be > 0'\n",
    "\n",
    "    for epoch, val_obj in early_stopping(model, stop_closure,\n",
    "                                         interval=interval, patience=patience,\n",
    "                                         start=epoch, max_iter=max_iter, maximize=maximize,\n",
    "                                         tolerance=tolerance, restore_best=restore_best):\n",
    "        for batch_no, (readout_key, *data) in \\\n",
    "                tqdm(enumerate(cycle_datasets(trainloaders, requires_grad=False, cuda=cuda)),\n",
    "                     desc='Training  | Epoch {}'.format(epoch)):\n",
    "            obj = objective(model, readout_key, *data)\n",
    "            obj.backward()\n",
    "            if iteration % accumulate_gradient == accumulate_gradient - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            iteration += 1\n",
    "\n",
    "        if post_epoch_hook is not None:\n",
    "            model = post_epoch_hook(model, epoch)\n",
    "    return model, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch = 0\n",
    "# --- train core, modulator, and readout but not shifter\n",
    "schedule = [0.005, 0.001]\n",
    "\n",
    "for opt, lr in zip(repeat(torch.optim.Adam), schedule):\n",
    "    print('Training with learning rate', lr)\n",
    "    optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "    model, epoch = train(model, full_objective, optimizer,\n",
    "                                   stop_closure, trainloaders,\n",
    "                                   epoch=epoch,\n",
    "                                   max_iter=100,\n",
    "                                   interval=4,\n",
    "                                   patience=4,\n",
    "                                   accumulate_gradient=acc\n",
    "                                   )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceScores = namedtuple('PerformanceScores', ['pearson'])\n",
    "\n",
    "\n",
    "def compute_scores(y, y_hat, axis=0):\n",
    "    pearson = corr(y, y_hat, axis=axis)\n",
    "    return PerformanceScores(pearson=pearson)\n",
    "\n",
    "def compute_predictions(loader, model, readout_key, reshape=True, stack=True, subsamp_size=None, return_lag=False):\n",
    "    y, y_hat = [], []\n",
    "    for x_val, beh_val, eye_val, y_val in tqdm(to_variable(loader, filter=(True, True, True, False),\n",
    "                                                           cuda=True, volatile=True), desc='predictions'):\n",
    "        neurons = y_val.size(-1)\n",
    "        if subsamp_size is None:\n",
    "            y_mod = model(x_val, readout_key, eye_pos=eye_val, behavior=beh_val).data.cpu().numpy()\n",
    "        else:\n",
    "            y_mod = []\n",
    "            neurons = y_val.size(-1)\n",
    "            for subs_idx in slice_iter(neurons, subsamp_size):\n",
    "                y_mod.append(\n",
    "                    model(x_val, readout_key, eye_pos=eye_val,\n",
    "                          behavior=beh_val, subs_idx=subs_idx).data.cpu().numpy())\n",
    "            y_mod = np.concatenate(y_mod, axis=-1)\n",
    "\n",
    "        lag = y_val.shape[1] - y_mod.shape[1]\n",
    "        if reshape:\n",
    "            y.append(y_val[:, lag:, :].numpy().reshape((-1, neurons)))\n",
    "            y_hat.append(y_mod.reshape((-1, neurons)))\n",
    "        else:\n",
    "            y.append(y_val[:, lag:, :].numpy())\n",
    "            y_hat.append(y_mod)\n",
    "    if stack:\n",
    "        y, y_hat = np.vstack(y), np.vstack(y_hat)\n",
    "    if not return_lag:\n",
    "        return y, y_hat\n",
    "    else:\n",
    "        return y, y_hat, lag\n",
    "        \n",
    "def compute_test_scores(testloaders, model, readout_key):\n",
    "    loader = testloaders[readout_key]\n",
    "\n",
    "    y, y_hat = compute_predictions(loader, model, readout_key, reshape=True, stack=True, subsamp_size=None)\n",
    "    return compute_scores(y, y_hat)  # scores is a named tuple\n",
    "\n",
    "\n",
    "\n",
    "def compute_test_score_tuples(key, testloaders, model):\n",
    "    scores, unit_scores = [], []\n",
    "    for readout_key, testloader in testloaders.items():\n",
    "        perf_scores = compute_test_scores(testloaders, model, readout_key)\n",
    "\n",
    "        member_key = (data.MovieMultiDataset.Member() & key & dict(name=readout_key)).fetch1(dj.key)  # get other fields\n",
    "        member_key.update(key)\n",
    "        unit_ids = testloader.dataset.neurons.unit_ids\n",
    "        member_key['neurons'] = len(unit_ids)\n",
    "        member_key['pearson'] = perf_scores.pearson.mean()\n",
    "\n",
    "        scores.append(member_key)\n",
    "        unit_scores.extend([dict(member_key, unit_id=u, pearson=c) for u, c in zip(unit_ids, perf_scores.pearson)])\n",
    "    return scores, unit_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, unit_scores = compute_test_score_tuples(key, testloaders, model)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
